{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbours\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple yet powerful supervised machine learning algorithm used primarily for classification and regression tasks. It operates on the principle that similar data points are close to each other in the feature space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How KNN Works**\n",
    "1. **Choose the Number of Neighbors (K)**: The first step is to determine the value of K, which represents the number of nearest neighbors to consider for making predictions.\n",
    "2. **Calculate Distance**: For a given test instance, calculate the distance from the test instance to all training instances using a distance metric. The most common metric is Euclidean distance, $d(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}$.   \n",
    "3. **Identify Neighbors**: Sort the distances and select the K closest instances from the training dataset.\n",
    "4. **Voting for Classification**: For classification tasks, the predicted class for the test instance is determined by majority voting among the K neighbors. For regression tasks, it is the average of the K neighbors' target values.\n",
    "\n",
    "\n",
    "> The value of K is typically chosen through experimentation, often using techniques such as cross-validation. A common strategy is to select an odd value for K to avoid ties in the voting process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN using the Iris Dataset\n",
    "\n",
    "- 4 features (numerical) \n",
    "    - Petal length\n",
    "    - Petal width\n",
    "    - Sepal length\n",
    "    - Sepal width\n",
    "- 1 target (categorical)\n",
    "    - Iris setosa\n",
    "    - Iris versicolor\n",
    "    - Iris virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import the libraries\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n",
      "[0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 2. Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "print(iris.data[:10])\n",
    "print(iris.target[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Separate the data into features and target\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Train-Test Split: Separate the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7) # 70% training, 30% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create the model\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Train the model\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: virginica, Actual: virginica\n",
      "Predicted: setosa, Actual: setosa\n",
      "Predicted: versicolor, Actual: versicolor\n",
      "Predicted: versicolor, Actual: versicolor\n",
      "Predicted: setosa, Actual: setosa\n",
      "Predicted: setosa, Actual: setosa\n",
      "Predicted: setosa, Actual: setosa\n",
      "Predicted: virginica, Actual: virginica\n",
      "Predicted: setosa, Actual: setosa\n",
      "Predicted: setosa, Actual: setosa\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "y_predict = classifier.predict(X_test)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Predicted: {iris.target_names[y_predict[i]]}, Actual: {iris.target_names[y_test[i]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics\n",
    "\n",
    "Evaluating the performance of a classification model by comparing its predictions with actual labels\n",
    "\n",
    "#### 1. Accuracy\n",
    "\n",
    "The ratio of correctly predicted instances to the total number of instances.\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "$$\n",
    "While simple and widely used, accuracy is misleading when dealing with imbalanced datasets where one class is much more frequent than the others.\n",
    "\n",
    "#### 2. Precision\n",
    "   \n",
    "The ratio of correctly predicted positive observations to the total predicted positives.\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}} \n",
    "$$\n",
    "\n",
    "Precision is important when the cost of false positives is high, as it shows how many of the positive predictions made by the model are actually correct.\n",
    "\n",
    "#### 3. Recall (Sensitivity or True Positive Rate)\n",
    "   \n",
    "The ratio of correctly predicted positive observations to all actual positives.\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} \n",
    "$$\n",
    "\n",
    "Recall is crucial when the cost of false negatives is high (e.g., in medical diagnoses), as it measures how well the model captures all actual positive cases.\n",
    "\n",
    "#### 4. F1 Score\n",
    "\n",
    "The harmonic mean of precision and recall.\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "F1 score provides a balance between precision and recall, especially useful in cases where an even balance is required. It’s especially useful for imbalanced classes.\n",
    "\n",
    "#### 5. Specificity (True Negative Rate)\n",
    "\n",
    "The ratio of correctly predicted negative observations to all actual negatives.\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{\\text{TN}}{\\text{TN} + \\text{FP}} \n",
    "$$\n",
    "\n",
    "Specificity is useful when the cost of false positives is high, focusing on the model’s ability to correctly identify negative cases.\n",
    "\n",
    "#### 6. ROC Curve and AUC (Area Under Curve)\n",
    "\n",
    "**ROC Curve** plots the True Positive Rate (Recall) against the False Positive Rate (1 - Specificity) at different threshold levels.\n",
    "**AUC (Area Under ROC Curve)** represents the overall ability of the model to distinguish between positive and negative classes.\n",
    "   \n",
    "The closer the AUC is to 1, the better the model is at correctly classifying positives and negatives across various thresholds. AUC is particularly useful in comparing multiple models.\n",
    "\n",
    "#### 7. Confusion Matrix\n",
    "\n",
    "A matrix displaying the counts of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) classifications.\n",
    "\n",
    "-|Predicted <span style=\"color:cyan\">P</span>ositive | Predicted <span style=\"color:orange\">N</span>egative\n",
    "---| --- | ---\n",
    "**Actual Positive** | <span style=\"color:green\">True <span style=\"color:cyan\">P</span>ositives</span> | <span style=\"color:red\">False <span style=\"color:orange\">N</span>egatives</span>\n",
    "**Actual Negative** | <span style=\"color:red\">False <span style=\"color:cyan\">P</span>ositives</span> | <span style=\"color:green\">True <span style=\"color:orange\">N</span>egatives</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Evaluate the model\n",
    "accuracy_score(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['versicolor']\n",
      "['setosa']\n",
      "['virginica']\n"
     ]
    }
   ],
   "source": [
    "# Make predictions \n",
    "result = classifier.predict([[3, 5, 4, 2]])\n",
    "print(iris.target_names[result])\n",
    "\n",
    "result = classifier.predict([[5, 4, 2, 1]])\n",
    "print(iris.target_names[result])\n",
    "\n",
    "result = classifier.predict([[4, 2, 5, 3]])\n",
    "print(iris.target_names[result])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is widely used in various applications, including:\n",
    "- **Recommendation Systems**: To recommend items based on user preferences.\n",
    "- **Image Classification**: For classifying images based on visual similarity.\n",
    "- **Anomaly Detection**: To identify outliers in datasets.\n",
    "- **Pattern Recognition**: Used in handwriting recognition and similar tasks.\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "|------------|---------------|\n",
    "| **Simplicity**: KNN is easy to understand and implement, making it a good choice for beginners. | **Computational Complexity**: KNN requires distance calculations between the test instance and all training instances, leading to high computational costs, especially with large datasets. |\n",
    "| **No Training Phase**: KNN does not require a separate training phase, as it simply stores the training data. | **Storage Requirements**: Since KNN stores the entire training dataset, it can consume a significant amount of memory. |\n",
    "| **Adaptability**: The algorithm can be adapted for both classification and regression tasks. | **Sensitivity to Noise**: KNN can be affected by noisy data and outliers, which may lead to incorrect predictions. |\n",
    "| **Flexibility**: KNN can handle multi-class classification problems and is also applicable to datasets with arbitrary shapes. | **Choosing K**: The choice of K can significantly affect performance. A small value may lead to overfitting, while a large value may smooth over the class boundaries. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
